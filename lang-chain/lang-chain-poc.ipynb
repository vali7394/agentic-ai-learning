{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# LangChain Tracing and Debugging : LangSmith\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The meaning of life is a philosophical and existential question that has been pondered by humans for centuries. Different cultures, religions, and individuals offer varied answers based on their beliefs and values. In general, possible interpretations include:\\n\\n1. **Religious or Spiritual Meaning**: Many religions offer explanations for the meaning of life, often tied to serving a higher power or following divine guidance.\\n\\n2. **Existential Perspectives**: Existentialist philosophy suggests that life has no inherent meaning, and it is up to individuals to give their own lives purpose through their actions and choices.\\n\\n3. **Scientific Views**: From a scientific standpoint, life can be seen as a natural process of evolution and survival, with meaning derived from the continuation and advancement of life.\\n\\n4. **Humanistic Views**: Some believe that the meaning of life is found in relationships, personal growth, and contributing to the well-being of others.\\n\\nUltimately, the meaning of life may be deeply personal and subjective, varying from person to person.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 14, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-af8e6abe-3139-4923-9a03-f9f18f9d0f6e-0', usage_metadata={'input_tokens': 14, 'output_tokens': 202, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "result = llm.invoke(\"What is the meaning of life?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The meaning of life is a philosophical and existential question that has been pondered by humans for centuries. Different cultures, religions, and individuals offer varied answers based on their beliefs and values. In general, possible interpretations include:\\n\\n1. **Religious or Spiritual Meaning**: Many religions offer explanations for the meaning of life, often tied to serving a higher power or following divine guidance.\\n\\n2. **Existential Perspectives**: Existentialist philosophy suggests that life has no inherent meaning, and it is up to individuals to give their own lives purpose through their actions and choices.\\n\\n3. **Scientific Views**: From a scientific standpoint, life can be seen as a natural process of evolution and survival, with meaning derived from the continuation and advancement of life.\\n\\n4. **Humanistic Views**: Some believe that the meaning of life is found in relationships, personal growth, and contributing to the well-being of others.\\n\\nUltimately, the meaning of life may be deeply personal and subjective, varying from person to person.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an expert A/B Testing Design engineer. Providing me with the answer to the questions like i'm new to A/B Testing\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts  import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "[\n",
    "(\"system\", \"You are an expert A/B Testing Design engineer. Providing me with the answer to the questions like i'm new to A/B Testing\"),\n",
    "(\"user\", \"{user_input}\")\n",
    "]\n",
    "\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are an expert A/B Testing Design engineer. Providing me with the answer to the questions like i'm new to A/B Testing\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x117543d40>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11756a120>, root_client=<openai.OpenAI object at 0x1173eee10>, root_async_client=<openai.AsyncOpenAI object at 0x10efa6690>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Designing the traffic split for an A/B test is an important step to ensure that you have a valid and statistically significant test. Here's how you can approach it:\\n\\n1. **Identify Your Variants:** Decide how many variants you will have in your test. At a basic level, you will have a control group (A) and one or more treatment groups (B, C, etc.).\\n\\n2. **Determine Your Objectives:** Understand what you aim to achieve with the test. This might influence how you want to allocate traffic, especially if some variants are riskier or if certain insights are prioritized.\\n\\n3. **Choose a Traffic Split:** Typically, traffic is evenly split between your control and each variation to ensure equality and reduce bias. For instance, if you have a control and one variant, a common split is 50/50. If you have more variants, you might consider a 33/33/33 split for three groups. However, in some cases, you might intentionally skew the distribution (e.g., 10/90) if you want to initially limit exposure to a treatment for risk management.\\n\\n4. **Consider Statistical Power:** Ensure that each group has enough participants to achieve statistical significance. This depends on the size of the effect you are trying to detect, the expected traffic, and the chosen confidence level (commonly 95%).\\n\\n5. **Account for External Constraints:** Sometimes, business or technical constraints might influence the split. For example, if a certain variant is costlier or if there's limited server capacity, you might allocate less traffic to that variant.\\n\\n6. **Run a Power Analysis:** This analysis helps you determine the minimum sample size required in each group to detect a meaningful difference. Tools and statistical calculators can assist you with this step.\\n\\n7. **Randomization:** Ensure that users are randomly assigned to each group to mitigate selection bias. This randomness helps maintain the integrity of your results.\\n\\n8. **Implement and Monitor:** Once your traffic split is determined, implement the test and closely monitor its progress. Be prepared to make adjustments if early data suggests an imbalance or unintended issue.\\n\\n9. **Document the Split:** Keep a record of your traffic split and the rationale behind it. This documentation will be important for analyzing results and communicating findings.\\n\\nBy following these steps, you will set up a robust traffic split for your A/B test, which is critical for obtaining reliable and actionable results.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 485, 'prompt_tokens': 49, 'total_tokens': 534, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-f0e7f36b-b63d-4852-a390-14714593b90c-0', usage_metadata={'input_tokens': 49, 'output_tokens': 485, 'total_tokens': 534, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response = chain.invoke({\"user_input\": \"How do you design the traffic split for an A/B test?\"})\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Designing the traffic split for an A/B test is an important step to ensure that you have a valid and statistically significant test. Here's how you can approach it:\\n\\n1. **Identify Your Variants:** Decide how many variants you will have in your test. At a basic level, you will have a control group (A) and one or more treatment groups (B, C, etc.).\\n\\n2. **Determine Your Objectives:** Understand what you aim to achieve with the test. This might influence how you want to allocate traffic, especially if some variants are riskier or if certain insights are prioritized.\\n\\n3. **Choose a Traffic Split:** Typically, traffic is evenly split between your control and each variation to ensure equality and reduce bias. For instance, if you have a control and one variant, a common split is 50/50. If you have more variants, you might consider a 33/33/33 split for three groups. However, in some cases, you might intentionally skew the distribution (e.g., 10/90) if you want to initially limit exposure to a treatment for risk management.\\n\\n4. **Consider Statistical Power:** Ensure that each group has enough participants to achieve statistical significance. This depends on the size of the effect you are trying to detect, the expected traffic, and the chosen confidence level (commonly 95%).\\n\\n5. **Account for External Constraints:** Sometimes, business or technical constraints might influence the split. For example, if a certain variant is costlier or if there's limited server capacity, you might allocate less traffic to that variant.\\n\\n6. **Run a Power Analysis:** This analysis helps you determine the minimum sample size required in each group to detect a meaningful difference. Tools and statistical calculators can assist you with this step.\\n\\n7. **Randomization:** Ensure that users are randomly assigned to each group to mitigate selection bias. This randomness helps maintain the integrity of your results.\\n\\n8. **Implement and Monitor:** Once your traffic split is determined, implement the test and closely monitor its progress. Be prepared to make adjustments if early data suggests an imbalance or unintended issue.\\n\\n9. **Document the Split:** Keep a record of your traffic split and the rationale behind it. This documentation will be important for analyzing results and communicating findings.\\n\\nBy following these steps, you will set up a robust traffic split for your A/B test, which is critical for obtaining reliable and actionable results.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Designing a traffic split for an A/B test involves determining how you will divide your users between the different variations or groups in your experiment. Here‚Äôs a step-by-step guide to help you design this aspect of your A/B test effectively:\\n\\n1. **Define Variations:**\\n   - Identify what variations you will be testing. Usually, you will have at least one control group (A) which is your current version and one or more test variations (B, C, etc.).\\n\\n2. **Determine Sample Size:**\\n   - Calculate the total number of users you will need to reach statistically significant results. Use statistical power analysis to ensure your sample size is large enough to detect a meaningful difference between variations.\\n   - Key components for calculating sample size include expected effect size, significance level (commonly set at 0.05), and statistical power (commonly set at 0.8).\\n\\n3. **Decide on Traffic Split:**\\n   - The most common approach is a 50/50 split between two variations (A and B) if you have a control and one test variant. This ensures maximum statistical power.\\n   - If you have more than two variations, you can split traffic evenly, e.g., 33/33/33 for three groups, or adjust based on business priorities or ethical considerations.\\n\\n4. **Random Assignment:**\\n   - Implement a method to randomly assign users to variations. It‚Äôs critical that this assignment is truly random to avoid biases.\\n   - Many A/B testing platforms handle this automatically; ensure yours does, or work with your development team to create or verify a robust assignment logic.\\n\\n5. **Ensure Even Distribution:**\\n   - Monitor your test to ensure that traffic is being split evenly or as per your design. Sometimes technical issues can cause uneven distribution, which you should rectify promptly.\\n\\n6. **Consider User Segmentation:**\\n   - Decide if all users will be included in the test or if you will segment users based on certain criteria (e.g., new vs. returning users). Ensure segmentation doesn‚Äôt introduce bias that invalidates the test.\\n\\n7. **Account for Any Constraints:**\\n   - If there are logistical or ethical constraints (e.g., needing to limit exposure to a risky variant), adjust your traffic split accordingly.\\n   - Ensure stakeholders understand the implications of any constraints on the test results.\\n\\n8. **Technical Implementation:**\\n   - Work with your technical team to implement the traffic split in your A/B testing platform. This may involve setting up rules for traffic routing, creating test segments, and ensuring tracking is properly coded.\\n\\n9. **Run a Pre-Test Check:**\\n   - Conduct a dry-run or smoke test to check the system, ensuring the traffic split and data collection are working as intended before full-scale deployment.\\n\\n10. **Documentation and Communication:**\\n    - Document the traffic split criteria, logic, and expected outcomes. Communicate this to stakeholders so everyone involved understands how the test is structured.\\n\\nBy following these steps, you can design a traffic split that ensures your A/B test will yield trustworthy and actionable insights.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "llm_response = chain.invoke({\"user_input\": \"Techinical design the traffic split for an A/B test?\"})\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\\n\\nGet started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "web_loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "document = web_loader.load()\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "document_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x118f6a960>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vector_store = FAISS.from_documents(document_chunks, embeddings)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c97c8fbe-d6ff-4a0a-bfb5-3b5d8046a16b', metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(id='98a8e1f1-dd6a-4f76-b048-86b9a2122f77', metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.'),\n",
       " Document(id='1c748eed-eae4-4c25-9a49-bda79accf50a', metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.'),\n",
       " Document(id='87ed74a5-25bd-412b-a1f2-bf6f700b9baf', metadata={'source': 'https://docs.smith.langchain.com/', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\")]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"StartedObservabilityEvaluationPrompt EngineeringDeployment\"\n",
    "ss_result = vector_store.similarity_search(query, top_k=1)\n",
    "ss_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\n",
    "\"\"\"\n",
    "\"Answer the following questions based on the provided context\"\n",
    "<context>\n",
    "{{context}}\n",
    "</context>\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
